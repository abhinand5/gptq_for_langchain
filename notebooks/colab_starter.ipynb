{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "BCSSNJpmh_IJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNDW8OPSaqWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3ac99c-5bba-4398-e110-5663e2d0095c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.1/789.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain huggingface_hub transformers sentence_transformers \n",
        "!pip install -q accelerate bitsandbytes safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsRd6-BwfKp2",
        "outputId": "afcce4a2-ed8d-484b-a95e-96cf76cfbc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWEtKL4ma8h3",
        "outputId": "2ce1e84d-4243-4444-ba33-6d16c1cdb6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 11 05:16:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/TheBloke/wizardLM-7B-GPTQ ./models/wizardLM-7B-GPTQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9CpIYcAfVlf",
        "outputId": "35a70167-bfed-4e7b-81b8-c77efcecc5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into './models/wizardLM-7B-GPTQ'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 100 (delta 51), reused 2 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (100/100), 492.56 KiB | 10.71 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Filtering content: 100% (3/3), 7.25 GiB | 52.54 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "# !cp ./GPTQ-for-LLaMa/setup_cuda.py ./GPTQ-for-LLaMa/setup.py\n",
        "# !cd GPTQ-for-LLaMa && python setup_cuda.py install\n",
        "!cd GPTQ-for-LLaMa && python3 setup_cuda.py bdist_wheel -d ."
      ],
      "metadata": {
        "id": "QxNHsjnblUvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ./GPTQ-for-LLaMa/*.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHFolKiR0Xd4",
        "outputId": "bb3bf758-775e-41d5-c99b-0336cda84fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./GPTQ-for-LLaMa/quant_cuda-0.0.0-cp310-cp310-linux_x86_64.whl\n",
            "Installing collected packages: quant-cuda\n",
            "Successfully installed quant-cuda-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** If you get `NameError: No module named quant_cuda`, restart the kernel and reinstall GPTQ in the above cell"
      ],
      "metadata": {
        "id": "w6tPK4bs295B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, str(Path(\"/content/GPTQ-for-LLaMa\")))\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import site \n",
        "import torch\n",
        "import logging\n",
        "import accelerate\n",
        "import inspect\n",
        "\n",
        "import numpy as np\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BitsAndBytesConfig,\n",
        "    LlamaTokenizer,\n",
        "    AutoConfig, \n",
        "    AutoModelForCausalLM\n",
        ")"
      ],
      "metadata": {
        "id": "epLYQ-Y_cHSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from modelutils import find_layers\n",
        "except ImportError:\n",
        "    from utils import find_layers\n",
        "\n",
        "\n",
        "try:\n",
        "    from quant import make_quant\n",
        "    is_triton = False\n",
        "except ImportError:\n",
        "    import quant\n",
        "    is_triton = True"
      ],
      "metadata": {
        "id": "XsLY_KFsl624"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPTQ_MODEL_DIR = \"/content/models/\"\n",
        "MODEL_NAME = \"wizardLM-7B-GPTQ\""
      ],
      "metadata": {
        "id": "C5eObyGDgBvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "BCSSNJpmh_IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on https://github.com/oobabooga/text-generation-webui/blob/main/modules/GPTQ_loader.py\n",
        "def find_quantized_model_file(model_name, args):\n",
        "    if args.checkpoint:\n",
        "        return Path(args.checkpoint)\n",
        "\n",
        "    path_to_model = Path(f'{args.model_dir}/{model_name}')\n",
        "    print(f\"Path to Model: {path_to_model}\")\n",
        "    pt_path = None\n",
        "    priority_name_list = [\n",
        "        Path(f'{args.model_dir}/{model_name}{hyphen}{args.wbits}bit{group}{ext}')\n",
        "        for group in ([f'-{args.groupsize}g', ''] if args.groupsize > 0 else [''])\n",
        "        for ext in ['.safetensors', '.pt']\n",
        "        for hyphen in ['-', f'/{model_name}-', '/']\n",
        "    ]\n",
        "    for path in priority_name_list:\n",
        "        if path.exists():\n",
        "            pt_path = path\n",
        "            break\n",
        "\n",
        "    # If the model hasn't been found with a well-behaved name, pick the last .pt\n",
        "    # or the last .safetensors found in its folder as a last resort\n",
        "    if not pt_path:\n",
        "        found_pts = list(path_to_model.glob(\"*.pt\"))\n",
        "        found_safetensors = list(path_to_model.glob(\"*.safetensors\"))\n",
        "        pt_path = None\n",
        "\n",
        "        if len(found_pts) > 0:\n",
        "            if len(found_pts) > 1:\n",
        "                logging.warning('More than one .pt model has been found. The last one will be selected. It could be wrong.')\n",
        "\n",
        "            pt_path = found_pts[-1]\n",
        "        elif len(found_safetensors) > 0:\n",
        "            if len(found_pts) > 1:\n",
        "                logging.warning('More than one .safetensors model has been found. The last one will be selected. It could be wrong.')\n",
        "\n",
        "            pt_path = found_safetensors[-1]\n",
        "\n",
        "    return pt_path"
      ],
      "metadata": {
        "id": "mKSrtJXZiXeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section Inspired from https://github.com/oobabooga/text-generation-webui\n",
        "def _load_quant(model, checkpoint, wbits, groupsize=-1, faster_kernel=False, eval=False, exclude_layers=['lm_head'], kernel_switch_threshold=128):\n",
        "    def noop(*args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model, trust_remote_code=True)\n",
        "    logging.info(f\"Model Config: {config}\")\n",
        "    \n",
        "    torch.nn.init.kaiming_uniform_ = noop\n",
        "    torch.nn.init.uniform_ = noop\n",
        "    torch.nn.init.normal_ = noop\n",
        "\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    transformers.modeling_utils._init_weights = False\n",
        "    torch.set_default_dtype(torch.half)\n",
        "    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
        "    torch.set_default_dtype(torch.float)\n",
        "    \n",
        "    if eval:\n",
        "        model = model.eval()\n",
        "\n",
        "    layers = find_layers(model)\n",
        "    for name in exclude_layers:\n",
        "        if name in layers:\n",
        "            del layers[name]\n",
        "\n",
        "    if not is_triton:\n",
        "        gptq_args = inspect.getfullargspec(make_quant).args\n",
        "        make_quant_kwargs = {\n",
        "                'module': model,\n",
        "                'names': layers,\n",
        "                'bits': wbits,\n",
        "            }\n",
        "        if 'groupsize' in gptq_args:\n",
        "            make_quant_kwargs['groupsize'] = groupsize\n",
        "        if 'faster' in gptq_args:\n",
        "            make_quant_kwargs['faster'] = faster_kernel\n",
        "        if 'kernel_switch_threshold' in gptq_args:\n",
        "            make_quant_kwargs['kernel_switch_threshold'] = kernel_switch_threshold\n",
        "        \n",
        "        make_quant(**make_quant_kwargs)\n",
        "    else:\n",
        "        logging.exception(\"Triton not supported!\")\n",
        "\n",
        "    del layers\n",
        "\n",
        "    if checkpoint.endswith('.safetensors'):\n",
        "        from safetensors.torch import load_file as safe_load\n",
        "        model.load_state_dict(safe_load(checkpoint), strict=False)\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(checkpoint), strict=False)\n",
        "\n",
        "    model.seqlen = 2048\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8RxQ2-yEpd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section Inspired from https://github.com/oobabooga/text-generation-webui\n",
        "def load_quantized_model(model_name, args, load_tokenizer=True):\n",
        "    tokenizer = None\n",
        "    path_to_model = Path(f'{args.model_dir}/{model_name}')\n",
        "    pt_path = find_quantized_model_file(model_name, args)\n",
        "    if not pt_path:\n",
        "        print(pt_path)\n",
        "        logging.error(\"Could not find the quantized model in .pt or .safetensors format, exiting...\")\n",
        "        return\n",
        "    else:\n",
        "        logging.info(f\"Found the following quantized model: {pt_path}\")\n",
        "    \n",
        "    threshold = args.threshold if args.threshold else 128\n",
        "\n",
        "    model = _load_quant(\n",
        "        str(path_to_model), \n",
        "        str(pt_path), \n",
        "        args.wbits, \n",
        "        args.groupsize, \n",
        "        kernel_switch_threshold=threshold\n",
        "    )\n",
        "\n",
        "    model = model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    if load_tokenizer:\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(\n",
        "            Path(f\"{args.model_dir}/{model_name}/\"), \n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            tokenizer.eos_token_id = 2\n",
        "            tokenizer.bos_token_id = 1\n",
        "            tokenizer.pad_token_id = 0\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "7NauoBtrhpKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ],
      "metadata": {
        "id": "rnqxSBFSpDIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WizardLM + Langchain"
      ],
      "metadata": {
        "id": "O60YDVrdqViC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"wbits\": 4,\n",
        "    \"groupsize\": 128,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"model_dir\": GPTQ_MODEL_DIR,\n",
        "}\n",
        "\n",
        "model, tokenizer = load_quantized_model(MODEL_NAME, args=AttributeDict(args))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDVI9p6uqSBz",
        "outputId": "2b0968d2-198f-4970-ea82-3a093d0b5960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to Model: /content/models/wizardLM-7B-GPTQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_memory = {\n",
        "    0: \"15360MiB\",\n",
        "    'cpu': \"12GiB\"\n",
        "}"
      ],
      "metadata": {
        "id": "XtSN1DgdrFu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_map = accelerate.infer_auto_device_map(\n",
        "    model, \n",
        "    max_memory=max_memory, \n",
        "    no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
        ")\n",
        "model = accelerate.dispatch_model(\n",
        "    model, \n",
        "    device_map=device_map, \n",
        "    offload_buffers=True\n",
        ")"
      ],
      "metadata": {
        "id": "J2Uq0-0NreP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_memory_footprint() / (1024 * 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmV3chyroqq",
        "outputId": "81f67a5e-772c-4919-b801-3a140289f638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3874.857421875"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=512,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=llm_pipeline)"
      ],
      "metadata": {
        "id": "Vri0WDjyrs8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2f78be-bc0b-4d2e-c84d-e5649827310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt, \n",
        "    llm=local_llm\n",
        ")"
      ],
      "metadata": {
        "id": "jnvhB3uuryde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run('Who invented the light bulb?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkG_TgGBry7K",
        "outputId": "07b1c18d-fcda-4b54-c015-3f7ce69480f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The light bulb is a device that produces light by using electricity. It was invented by many people over the years, but the most famous inventor of the light bulb is Thomas Edison. He invented the first practical and commercially successful light bulb in 1879.\n"
          ]
        }
      ]
    }
  ]
}