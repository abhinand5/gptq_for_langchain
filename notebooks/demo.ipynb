{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abhinand/miniconda3/envs/wizardlm_langchain/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Could not import azure.core python package.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/abhinand/dev/ai/wizardlm_langchain\")\n",
        "\n",
        "from wizardlm_langchain.model import load_quantized_model\n",
        "from wizardlm_langchain.helpers import AttributeDict, get_available_memory\n",
        "\n",
        "import accelerate\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPTQ_MODEL_DIR = \"../models/\"\n",
        "MODEL_NAME = \"wizardLM-7B-GPTQ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDVI9p6uqSBz",
        "outputId": "2b0968d2-198f-4970-ea82-3a093d0b5960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to Model: ../models/wizardLM-7B-GPTQ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abhinand/miniconda3/envs/wizardlm_langchain/lib/python3.10/site-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "/home/abhinand/miniconda3/envs/wizardlm_langchain/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/home/abhinand/miniconda3/envs/wizardlm_langchain/lib/python3.10/site-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n"
          ]
        }
      ],
      "source": [
        "args = {\n",
        "    \"wbits\": 4,\n",
        "    \"groupsize\": 128,\n",
        "    \"model_type\": \"llama\",\n",
        "    \"model_dir\": GPTQ_MODEL_DIR,\n",
        "}\n",
        "\n",
        "model, tokenizer = load_quantized_model(MODEL_NAME, args=AttributeDict(args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XtSN1DgdrFu9"
      },
      "outputs": [],
      "source": [
        "# max_memory = {\n",
        "#     0: \"15360MiB\",\n",
        "#     'cpu': \"12GiB\"\n",
        "# }\n",
        "\n",
        "max_memory = {\n",
        "    0: 6000000000,\n",
        "    'cpu': 13000000000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J2Uq0-0NreP6"
      },
      "outputs": [],
      "source": [
        "device_map = accelerate.infer_auto_device_map(\n",
        "    model, \n",
        "    max_memory=max_memory, \n",
        "    no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
        ")\n",
        "model = accelerate.dispatch_model(\n",
        "    model, \n",
        "    device_map=device_map, \n",
        "    offload_buffers=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmV3chyroqq",
        "outputId": "81f67a5e-772c-4919-b801-3a140289f638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3874.857421875"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_memory_footprint() / (1024 * 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vri0WDjyrs8o",
        "outputId": "2a2f78be-bc0b-4d2e-c84d-e5649827310e"
      },
      "outputs": [],
      "source": [
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=512,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=llm_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jnvhB3uuryde"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt, \n",
        "    llm=local_llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkG_TgGBry7K",
        "outputId": "07b1c18d-fcda-4b54-c015-3f7ce69480f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The light bulb is a device that produces light by using electricity. It was invented by many people over the years, but the most famous inventor of the light bulb is Thomas Edison. He invented the first practical and commercially successful light bulb in 1879.\n"
          ]
        }
      ],
      "source": [
        "print(llm_chain.run('Who invented the light bulb?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The Treaty of Versailles, which ended World War 1, imposed harsh penalties on Germany, including the payment of reparations and the loss of territory. Germany was in economic crisis and faced political unrest, which led to the rise of Adolf Hitler and the Nazi Party in Germany. Hitler wanted to regain lost territory and restore German pride, which led to his aggressive foreign policy and the expansion of Nazi Germany. This caused tension and conflict with neighboring countries, ultimately leading to the outbreak of World War 2 in 1939.\n"
          ]
        }
      ],
      "source": [
        "print(llm_chain.run('What started the World War 2?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The Australian bush fires happened in the summer of 2019-2020.\n"
          ]
        }
      ],
      "source": [
        "print(llm_chain.run('When did the Australian bush fires happen?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Almonds are a type of nut, which means they are a fruit that contains a single seed. Fruits are typically high in fiber and low in fat. However, almonds are also a good source of healthy fats, specifically monounsaturated and polyunsaturated fats. \n",
            "\n",
            "One ounce of almonds (about 28 nuts) contains around 16 grams of fat, with around 13 grams of that being healthy monounsaturated fats. So, while almonds are not a particularly high-fat food, the fat they do contain is mostly healthy and beneficial for your body.\n"
          ]
        }
      ],
      "source": [
        "print(llm_chain.run('How much fat does almonds have?'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BCSSNJpmh_IJ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
